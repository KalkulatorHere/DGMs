{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iytJgDmslJs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicVAE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, z_dim):\n",
        "        super(BasicVAE, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dims[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[2], z_dim * 2)  # for mean and log-variance\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, hidden_dims[2]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[2], hidden_dims[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[1], hidden_dims[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[0], input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        mu, logvar = x.chunk(2, dim=-1)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n"
      ],
      "metadata": {
        "id": "57dFfpkxX44f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncDecVAE_Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, z_dim):\n",
        "        super(EncDecVAE_Encoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dims[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder within Encoder\n",
        "        self.encoder_decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_dims[2], hidden_dims[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[1], hidden_dims[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[0], z_dim * 2)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, hidden_dims[2]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[2], hidden_dims[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[1], hidden_dims[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[0], input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.encoder_decoder(x)\n",
        "        mu, logvar = x.chunk(2, dim=-1)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n"
      ],
      "metadata": {
        "id": "qua5EXjBsoNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncDecVAE_Decoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, z_dim):\n",
        "        super(EncDecVAE_Decoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dims[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[2], z_dim * 2)  # for mean and log-variance\n",
        "        )\n",
        "\n",
        "        # Encoder within Decoder\n",
        "        self.decoder_encoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, hidden_dims[2]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[2], hidden_dims[1]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[2], hidden_dims[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[1], hidden_dims[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[0], input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        mu, logvar = x.chunk(2, dim=-1)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = self.decoder_encoder(z)\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n"
      ],
      "metadata": {
        "id": "tnz4TytWsuQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncDecVAE_Both(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, z_dim):\n",
        "        super(EncDecVAE_Both, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dims[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder within Encoder\n",
        "        self.encoder_decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_dims[2], hidden_dims[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[1], hidden_dims[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[0], z_dim * 2)\n",
        "        )\n",
        "\n",
        "        # Encoder within Decoder\n",
        "        self.decoder_encoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, hidden_dims[2]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[2], hidden_dims[1]),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(hidden_dims[1], hidden_dims[2]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[2], hidden_dims[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[1], hidden_dims[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dims[0], input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.encoder_decoder(x)\n",
        "        mu, logvar = x.chunk(2, dim=-1)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = self.decoder_encoder(z)\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n"
      ],
      "metadata": {
        "id": "z5MjM3-8sxdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n"
      ],
      "metadata": {
        "id": "xGR0nnTztLF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a transform to normalize the data to [0, 1]\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n"
      ],
      "metadata": {
        "id": "mS35cR5eX-Ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust the normalization to [0, 1]\n",
        "class NormalizeTo01:\n",
        "    def __call__(self, tensor):\n",
        "        return (tensor + 1) / 2\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    NormalizeTo01()\n",
        "])\n"
      ],
      "metadata": {
        "id": "-TaHNNMNYAJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and load the training data\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n"
      ],
      "metadata": {
        "id": "aNq1jTZ1YBUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define the loss function\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Define the training loop\n",
        "def train_vae(model, dataloader, optimizer, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0\n",
        "        for batch_idx, (data, _) in enumerate(dataloader):\n",
        "            data = data.view(data.size(0), -1)  # Flatten the images\n",
        "            optimizer.zero_grad()\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            loss = loss_function(recon_batch, data, mu, logvar)\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch {epoch + 1}, Loss: {train_loss / len(dataloader.dataset)}')\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_vae(model, dataloader):\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, _) in enumerate(dataloader):\n",
        "            data = data.view(data.size(0), -1)\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            eval_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
        "    avg_loss = eval_loss / len(dataloader.dataset)\n",
        "    print(f'Average Loss: {avg_loss}')\n",
        "    return avg_loss\n",
        "\n",
        "# Train and evaluate the basic VAE\n",
        "input_dim = 784  # 28x28 images\n",
        "hidden_dims = [400, 200, 100]\n",
        "z_dim = 20\n",
        "epochs = 10\n",
        "\n",
        "# Initialize the model, optimizer\n",
        "basic_vae = BasicVAE(input_dim=input_dim, hidden_dims=hidden_dims, z_dim=z_dim)\n",
        "optimizer = optim.Adam(basic_vae.parameters(), lr=1e-3)\n",
        "\n",
        "# Train the model\n",
        "train_vae(basic_vae, train_loader, optimizer, epochs)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Evaluating Basic VAE\")\n",
        "evaluate_vae(basic_vae, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pG_-KPOIs0Km",
        "outputId": "580af17a-1cd1-4d47-86ca-a6fade24ebf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 525.2857182942709\n",
            "Epoch 2, Loss: 523.2812800130208\n",
            "Epoch 3, Loss: 522.7997369140625\n",
            "Epoch 4, Loss: 522.1077425130209\n",
            "Epoch 5, Loss: 521.1180700520833\n",
            "Epoch 6, Loss: 520.39628125\n",
            "Epoch 7, Loss: 519.6862736979167\n",
            "Epoch 8, Loss: 518.85702421875\n",
            "Epoch 9, Loss: 518.2068593098959\n",
            "Epoch 10, Loss: 517.7478166666667\n",
            "Evaluating Basic VAE\n",
            "Average Loss: 516.5412401367188\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "516.5412401367188"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate EncDecVAE_Encoder\n",
        "encdec_vae_encoder = EncDecVAE_Encoder(input_dim=input_dim, hidden_dims=hidden_dims, z_dim=z_dim)\n",
        "optimizer = optim.Adam(encdec_vae_encoder.parameters(), lr=1e-3)\n",
        "\n",
        "train_vae(encdec_vae_encoder, train_loader, optimizer, epochs)\n",
        "print(\"Evaluating EncDecVAE_Encoder\")\n",
        "evaluate_vae(encdec_vae_encoder, test_loader)\n",
        "\n",
        "# Train and evaluate EncDecVAE_Decoder\n",
        "encdec_vae_decoder = EncDecVAE_Decoder(input_dim=input_dim, hidden_dims=hidden_dims, z_dim=z_dim)\n",
        "optimizer = optim.Adam(encdec_vae_decoder.parameters(), lr=1e-3)\n",
        "\n",
        "train_vae(encdec_vae_decoder, train_loader, optimizer, epochs)\n",
        "print(\"Evaluating EncDecVAE_Decoder\")\n",
        "evaluate_vae(encdec_vae_decoder, test_loader)\n",
        "\n",
        "# Train and evaluate EncDecVAE_Both\n",
        "encdec_vae_both = EncDecVAE_Both(input_dim=input_dim, hidden_dims=hidden_dims, z_dim=z_dim)\n",
        "optimizer = optim.Adam(encdec_vae_both.parameters(), lr=1e-3)\n",
        "\n",
        "train_vae(encdec_vae_both, train_loader, optimizer, epochs)\n",
        "print(\"Evaluating EncDecVAE_Both\")\n",
        "evaluate_vae(encdec_vae_both, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srLKjlVguNkT",
        "outputId": "6bde2d1b-3801-47d7-c6f3-1cb6152b42aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 525.0125225260417\n",
            "Epoch 2, Loss: 523.1357188151042\n",
            "Epoch 3, Loss: 522.668647265625\n",
            "Epoch 4, Loss: 522.3084804036458\n",
            "Epoch 5, Loss: 522.2341761067709\n",
            "Epoch 6, Loss: 522.5656405598959\n",
            "Epoch 7, Loss: 521.963316796875\n",
            "Epoch 8, Loss: 521.52902265625\n",
            "Epoch 9, Loss: 521.05743046875\n",
            "Epoch 10, Loss: 520.337030859375\n",
            "Evaluating EncDecVAE_Encoder\n",
            "Average Loss: 519.0538999511718\n",
            "Epoch 1, Loss: 525.4752927083333\n",
            "Epoch 2, Loss: 523.578536328125\n",
            "Epoch 3, Loss: 522.5085984375\n",
            "Epoch 4, Loss: 521.57775546875\n",
            "Epoch 5, Loss: 520.7263766927083\n",
            "Epoch 6, Loss: 520.3041289713542\n",
            "Epoch 7, Loss: 519.9044121744791\n",
            "Epoch 8, Loss: 519.4789244791667\n",
            "Epoch 9, Loss: 519.1372115885416\n",
            "Epoch 10, Loss: 518.9431379557292\n",
            "Evaluating EncDecVAE_Decoder\n",
            "Average Loss: 517.7775861328125\n",
            "Epoch 1, Loss: 525.3061408203125\n",
            "Epoch 2, Loss: 523.1367444010417\n",
            "Epoch 3, Loss: 522.6147083333333\n",
            "Epoch 4, Loss: 522.1848690104167\n",
            "Epoch 5, Loss: 521.8696764973959\n",
            "Epoch 6, Loss: 521.7331504557292\n",
            "Epoch 7, Loss: 521.7667887369792\n",
            "Epoch 8, Loss: 522.3969543619792\n",
            "Epoch 9, Loss: 522.8125150390625\n",
            "Epoch 10, Loss: 523.2273961588542\n",
            "Evaluating EncDecVAE_Both\n",
            "Average Loss: 523.8562215820313\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "523.8562215820313"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}